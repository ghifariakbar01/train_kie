Good question ‚Äî **data imbalance is very common in KIE**, and handling it correctly can noticeably improve **recall and F1**, especially for rare labels like `B-TOTAL`, `B-DATE`, etc.

I‚Äôll explain **what ‚Äúbalancing‚Äù means in KIE**, **what you can and cannot balance**, and then give you **practical ways to apply it to your existing script**.

---

## 1Ô∏è‚É£ What ‚Äúdata imbalance‚Äù means in KIE (important)

In **token classification KIE**, imbalance exists at **token level**, not image level.

Typical situation:

* `O` label = **80‚Äì95%** of tokens
* `B-ITEM`, `B-PRICE` = many tokens
* `B-TOTAL`, `B-DATE`, `B-RECEIPT_NO` = very few tokens

So the model learns:

> ‚ÄúPredict `O` most of the time and I‚Äôll get good loss‚Äù

This **hurts recall** on rare fields.

‚ö†Ô∏è **You cannot simply oversample images** and expect it to fix token imbalance.
You need **label-aware strategies**.

---

## 2Ô∏è‚É£ What balancing methods work for KIE (ranked by impact)

### ‚úÖ 1. **Loss weighting (MOST IMPORTANT)**

This is the **correct and safest** method.

You tell the model:

> ‚ÄúMistakes on rare labels are more expensive.‚Äù

### ‚úÖ 2. **Targeted data augmentation**

You already do this (bbox jitter, token dropout) ‚Äî but we can **apply it selectively**.

### ‚ö†Ô∏è 3. **Oversampling documents**

Helps slightly, but risky if done blindly.

### ‚ùå 4. Removing `O` labels

Never do this ‚Äî the model must learn background.

---

## 3Ô∏è‚É£ Method 1 ‚Äî Class-weighted loss (RECOMMENDED)

### Step A ‚Äî Compute label frequencies (once)

Create a small script (or add to training setup):

```python
from collections import Counter
import numpy as np

def compute_label_weights(dataset, num_labels):
    counter = Counter()
    for ex in dataset:
        labels = ex["labels"]
        for l in labels:
            if l != -100:
                counter[l] += 1

    total = sum(counter.values())
    weights = np.zeros(num_labels, dtype=np.float32)

    for i in range(num_labels):
        freq = counter.get(i, 1)
        weights[i] = total / freq

    # normalize
    weights = weights / weights.mean()
    return weights
```

---

### Step B ‚Äî Use weighted CrossEntropyLoss

In **train.py**, replace the default loss by **subclassing the model**:

```python
import torch.nn as nn
from transformers import LayoutLMv3ForTokenClassification

class WeightedLayoutLMv3(LayoutLMv3ForTokenClassification):
    def __init__(self, config, class_weights):
        super().__init__(config)
        self.loss_fct = nn.CrossEntropyLoss(
            weight=torch.tensor(class_weights),
            ignore_index=-100
        )

    def forward(self, **kwargs):
        labels = kwargs.get("labels")
        outputs = super().forward(**kwargs)
        if labels is not None:
            logits = outputs.logits
            loss = self.loss_fct(
                logits.view(-1, self.num_labels),
                labels.view(-1)
            )
            outputs.loss = loss
        return outputs
```

---

### Step C ‚Äî Plug it into your training script

Replace this:

```python
model = LayoutLMv3ForTokenClassification.from_pretrained(...)
```

With:

```python
weights = compute_label_weights(train_ds, num_labels=len(id2label))

model = WeightedLayoutLMv3.from_pretrained(
    args.model_name,
    num_labels=len(id2label),
    id2label=id2label,
    label2id={v: k for k, v in id2label.items()},
    class_weights=weights
)
```

‚úÖ **This alone often improves recall + F1 by 5‚Äì15%** for rare fields.

---

## 4Ô∏è‚É£ Method 2 ‚Äî Noise augmentation only on rare-label pages

You already have **noise-aware collators**. Improve them:

### Idea

Apply strong augmentation **only when rare labels appear**.

```python
def has_rare_label(labels, rare_ids):
    return any(l in rare_ids for l in labels if l != -100)
```

Inside your collator:

```python
if has_rare_label(labels, rare_ids):
    bbox_jitter_prob = 0.8
    token_dropout_prob = 0.2
else:
    bbox_jitter_prob = 0.2
```

‚úÖ Makes the model **robust where it matters most**.

---

## 5Ô∏è‚É£ Method 3 ‚Äî Document oversampling (optional)

Only oversample documents that contain **rare entities**.

```python
def contains_label(example, target_ids):
    return any(l in target_ids for l in example["labels"])

rare_docs = train_ds.filter(lambda x: contains_label(x, rare_ids))
balanced_train = concatenate_datasets([train_ds, rare_docs])
```

‚ö†Ô∏è Use **small duplication (1‚Äì2√ó only)**.

---

## 6Ô∏è‚É£ What NOT to do ‚ùå

‚ùå Random SMOTE ‚Äî does not work for sequences
‚ùå Removing `O` labels
‚ùå Equalizing token counts manually

These **break sequence structure**.

---

## 7Ô∏è‚É£ How to verify balancing worked

After training, check:

| Metric    | Expected Change  |
| --------- | ---------------- |
| Loss      | Slightly higher  |
| Precision | Slight drop      |
| Recall    | **Big increase** |
| F1        | **Net increase** |

Specifically check **per-label recall**:

```python
eval_results["eval_recall_per_label"]
```

Rare labels should improve the most.

---

## 8Ô∏è‚É£ Minimal recommendation for your setup

Given your **RTX 2060 + current scripts**, do **only this first**:

1. ‚úÖ Add **class-weighted loss**
2. ‚úÖ Keep noise-aware collator
3. ‚ùå Skip oversampling initially

This is **safe, clean, and thesis-worthy**.

---

If you want, I can:

* Add **exact code patches** into your current `train.py`
* Help you **report balanced vs unbalanced results** properly in your thesis
* Show **per-label confusion analysis**

Just tell me üëç
